{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having a dense label to supervise the heatmap, we now just have a single action that was executed we can provide feedback for (poteially 1-4 depending on number of arms, etc.)\n",
    "\n",
    "See discussion with chat: https://chatgpt.com/c/68b98e84-ba50-8328-a9e5-088a8882eda4\n",
    "\n",
    "From the tossing bot paper:\n",
    "\n",
    "\"We pass gradients only through the single pixel i on which the grasping primitive was executed. All other pixels backpropagate with 0 loss.\"\n",
    "\n",
    "##### Key ideas:\n",
    " - Instead of applying BCE loss over the entire (C, H, W) tensor, select logits at the executed (c, h, w) indices.\n",
    " - This ensures the computational graph only flows through those selected entries, so gradients elsewhere are zero.\n",
    " - To handle multiple executed actions per sample, gather all relevant indices, compute the loss for each, and average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example shapes\n",
    "B, C, H, W = 4, 12, 64, 64  # batch, angles, image height, width\n",
    "logits = torch.randn(B, C, H, W, requires_grad=True)  # model output\n",
    "# Suppose ground truth labels (binary) for selected actions\n",
    "labels = torch.randint(0, 2, (B,), dtype=torch.float32)\n",
    "\n",
    "# Indices of the executed actions (batch-wise)\n",
    "# each row: (c, h, w) where the action was executed\n",
    "executed_inds = torch.tensor([\n",
    "    [3, 20, 15],\n",
    "    [7, 40, 10],\n",
    "    [0, 12, 32],\n",
    "    [5, 50, 50],\n",
    "])\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "loss_fn_mean = nn.BCEWithLogitsLoss(reduction='mean') # for some reason chat did 'none' but we see that its computing the same as if mean was used...\n",
    "\n",
    "# Gather the logits and targets at the executed indices\n",
    "b_indices = torch.arange(B)\n",
    "c_indices = executed_inds[:, 0]\n",
    "h_indices = executed_inds[:, 1]\n",
    "w_indices = executed_inds[:, 2]\n",
    "\n",
    "# Select only the executed action logits\n",
    "selected_logits = logits[b_indices, c_indices, h_indices, w_indices]\n",
    "selected_labels = labels  # shape [B]\n",
    "\n",
    "print(\"Selected logits shape:\", selected_logits.shape)\n",
    "print(\"Selected labels shape:\", selected_labels.shape)\n",
    "\n",
    "# Compute BCE loss only on those selected logits\n",
    "loss_per_sample = loss_fn(selected_logits, selected_labels)\n",
    "loss_per_sample_mean = loss_fn_mean(selected_logits, selected_labels)\n",
    "print(loss_per_sample_mean.shape)\n",
    "print(loss_per_sample.shape)\n",
    "loss = loss_per_sample.mean()\n",
    "\n",
    "loss.backward()\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Loss:\", loss_per_sample_mean.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extend this idea to work for k executed actions per heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# This assumes that the same number of actions are executed per batch.\n",
    "# Hasn't been tested yet, but saving here from chats handy work.\n",
    "\n",
    "def masked_bce_fixed_k(\n",
    "    logits: torch.Tensor,              # [B, C, H, W]\n",
    "    executed_inds: torch.Tensor,       # [B, K, 3]  (c, h, w) per action\n",
    "    labels: torch.Tensor,              # [B, K]     (0/1) per action\n",
    "    pos_weight: torch.Tensor | None = None,  # optional for BCEWithLogitsLoss\n",
    "    reduce: str = \"mean\",              # \"mean\" | \"sum\" | \"none\"\n",
    "):\n",
    "    B, C, H, W = logits.shape\n",
    "    B2, K, _ = executed_inds.shape\n",
    "    assert B2 == B and executed_inds.shape[-1] == 3\n",
    "\n",
    "    b = torch.arange(B, device=logits.device).unsqueeze(1).expand(B, K)  # [B,K]\n",
    "    c = executed_inds[..., 0]\n",
    "    h = executed_inds[..., 1]\n",
    "    w = executed_inds[..., 2]\n",
    "\n",
    "    # Select only the executed action logits -> [B, K]\n",
    "    selected_logits = logits[b, c, h, w]\n",
    "\n",
    "    # Compute BCE on those selected entries only\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=pos_weight)\n",
    "    per_action_loss = loss_fn(selected_logits, labels)\n",
    "\n",
    "    if reduce == \"mean\":\n",
    "        return per_action_loss.mean()\n",
    "    elif reduce == \"sum\":\n",
    "        return per_action_loss.sum()\n",
    "    else:\n",
    "        return per_action_loss  # [B, K]\n",
    "\n",
    "# --- Example usage ---\n",
    "B, C, H, W, K = 4, 12, 64, 64, 3\n",
    "logits = torch.randn(B, C, H, W, requires_grad=True)\n",
    "executed_inds = torch.tensor([\n",
    "    [[3,20,15],[3,21,15],[7,40,10]],\n",
    "    [[7,40,10],[7,41,10],[7,42,10]],\n",
    "    [[0,12,32],[0,13,32],[1,12,31]],\n",
    "    [[5,50,50],[5,51,50],[5,52,50]],\n",
    "])\n",
    "labels = torch.tensor([[1,0,1],[0,1,1],[1,1,0],[0,0,1]], dtype=torch.float32)\n",
    "\n",
    "loss = masked_bce_fixed_k(logits, executed_inds, labels, reduce=\"mean\")\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# this supports doing different number of actions per batch. \n",
    "def masked_BCEWithLogitsLoss(\n",
    "    logits: torch.Tensor,                         # [B, C, H, W]\n",
    "    executed_list: list[torch.Tensor],            # length B, each [Ki, 3] (c,h,w) (where Ki is the number of actions executed in batch i)\n",
    "    labels_list: list[torch.Tensor],              # length B, each [Ki] (0/1)\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "\n",
    "    \"\"\" \n",
    "        From the tossing bot paper: \"We pass gradients only through the single pixel i on which the grasping primitive was executed. All other pixels backpropagate with 0 loss.\"\n",
    "\n",
    "        ##### Key ideas to implement:\n",
    "        - Instead of applying BCE loss over the entire (C, H, W) tensor, select logits at the executed (c, h, w) indices.\n",
    "        - This ensures the computational graph only flows through those selected entries, so gradients elsewhere are zero.\n",
    "        - To handle multiple executed actions per sample, gather all relevant indices, compute the loss for each, and average.\n",
    "\n",
    "\n",
    "        ##### Notes\n",
    "\n",
    "\n",
    "        Currently lists look something like:\n",
    "\n",
    "        executed_list = [\n",
    "            [[3, 20, 15], [7, 40, 10]],    # B0 x 2\n",
    "            [[7, 40, 10]],                 # B1 x 1\n",
    "            [],                            # B2 x 0 (no action)\n",
    "            [[5, 50, 50], [5, 51, 50], [5, 52, 50]],  # B3 x 3\n",
    "        ]\n",
    "        labels_list = [\n",
    "            [1, 0],\n",
    "            [1],\n",
    "            [],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "\n",
    "        We want to convert into a single list to easily index the logics directly with a single look up.\n",
    "\n",
    "        b_all = [0, 0, 1, 3, 3, 3] # Notice how batch 2 disappears..\n",
    "        c_all = [3, 7, 7, 5, 5, 5]\n",
    "        h_all = [20, 40, 40, 50, 51, 52]\n",
    "        w_all = [15, 10, 10, 50, 50, 50]\n",
    "        y_all = [1, 0, 1, 0, 0, 1]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    B, C, H, W = logits.shape\n",
    "    device = logits.device\n",
    "\n",
    "\n",
    "\n",
    "    b_all, c_all, h_all, w_all, y_all, img_id = [], [], [], [], [], []\n",
    "    for i, (inds_i, y_i) in enumerate(zip(executed_list, labels_list)):\n",
    "        if inds_i is None or len(inds_i) == 0: # if no actions executed, the skip\n",
    "            continue\n",
    "        inds_i = inds_i.to(device)        # [Ki, 3]\n",
    "        y_i = y_i.to(device).float()      # [Ki]\n",
    "        Ki = inds_i.shape[0]\n",
    "\n",
    "        b_all.append(torch.full((Ki,), i, device=device, dtype=torch.long))\n",
    "        c_all.append(inds_i[:, 0].long())\n",
    "        h_all.append(inds_i[:, 1].long())\n",
    "        w_all.append(inds_i[:, 2].long())\n",
    "        y_all.append(y_i)\n",
    "\n",
    "    if len(b_all) == 0: # no actions executed this step\n",
    "        return logits.new_tensor(0.0, requires_grad=True)\n",
    "\n",
    "    b_all = torch.cat(b_all)  # [N]\n",
    "    c_all = torch.cat(c_all)  # [N]\n",
    "    h_all = torch.cat(h_all)  # [N]\n",
    "    w_all = torch.cat(w_all)  # [N]\n",
    "    y_all = torch.cat(y_all)  # [N]\n",
    "\n",
    "    selected_logits = logits[b_all, c_all, h_all, w_all]  # [N]\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    loss = loss_fn(selected_logits, y_all)  # [N]\n",
    "\n",
    "    info = {\n",
    "        'b_all': b_all,\n",
    "        'c_all': c_all,\n",
    "        'h_all': h_all,\n",
    "        'w_all': w_all,\n",
    "        'y_all': y_all,\n",
    "    }\n",
    "\n",
    "    return loss, info\n",
    "\n",
    "    #  Some ideas here if you wanted to do action weighting...\n",
    "\n",
    "    # loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=pos_weight)\n",
    "    # per_action = loss_fn(selected_logits, y_all)  # [N]\n",
    "\n",
    "    # if equal_weight_per_image:\n",
    "    #     # Average actions inside each image, then mean across images that had â‰¥1 action\n",
    "    #     loss_per_img = []\n",
    "    #     for i in range(B):\n",
    "    #         mask_i = (img_id == i)\n",
    "    #         if mask_i.any():\n",
    "    #             loss_per_img.append(per_action[mask_i].mean())\n",
    "    #     return torch.stack(loss_per_img).mean()\n",
    "    # else:\n",
    "    #     # All actions across batch equally weighted\n",
    "    #     return per_action.mean()\n",
    "\n",
    "# --- Example usage ---\n",
    "B, C, H, W = 4, 12, 64, 64\n",
    "logits = torch.randn(B, C, H, W, requires_grad=True)\n",
    "\n",
    "executed_list = [\n",
    "    torch.tensor([[3,20,15],[7,40,10]]),   # K0=2\n",
    "    torch.tensor([[7,40,10]]),             # K1=1\n",
    "    torch.tensor([], dtype=torch.long).reshape(0,3),  # K2=0 (no action)\n",
    "    torch.tensor([[5,50,50],[5,51,50],[5,52,50]]),    # K3=3\n",
    "]\n",
    "labels_list = [\n",
    "    torch.tensor([1, 0], dtype=torch.float32),\n",
    "    torch.tensor([1], dtype=torch.float32),\n",
    "    torch.tensor([], dtype=torch.float32),\n",
    "    torch.tensor([0, 0, 1], dtype=torch.float32),\n",
    "]\n",
    "\n",
    "loss, info = masked_BCEWithLogitsLoss(logits, executed_list, labels_list)\n",
    "\n",
    "loss.backward()\n",
    "print(info)\n",
    "\n",
    "expected_values = dict()\n",
    "expected_values['b_all'] = torch.tensor([0, 0, 1, 3, 3, 3])\n",
    "expected_values['c_all'] = torch.tensor([3, 7, 7, 5, 5, 5])\n",
    "expected_values['h_all'] = torch.tensor([20, 40, 40, 50, 51, 52])\n",
    "expected_values['w_all'] = torch.tensor([15, 10, 10, 50, 50, 50])\n",
    "expected_values['y_all'] = torch.tensor([1, 0, 1, 0, 0, 1])\n",
    "\n",
    "for key, value in info.items():\n",
    "    print(key, value.numpy())\n",
    "    assert torch.all(value == expected_values[key]), f\"Expected {key} to be {expected_values[key]}, but got {value}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will turn into a class now so that it can be used in more of the traditional flow of define and then use, instead of having to create the new loss funciton each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Dict\n",
    "\n",
    "class MaskedBCEWithLogitsLoss(nn.BCEWithLogitsLoss):\n",
    "    \"\"\"\n",
    "    BCEWithLogits computed ONLY at executed (c,h,w) indices per batch element.\n",
    "\n",
    "    Forward args:\n",
    "        logits:        [B, C, H, W]\n",
    "        executed_list: list length B, each a LongTensor [Ki, 3] with (c,h,w)\n",
    "        labels_list:   list length B, each a Float/Bool Tensor [Ki] with {0,1}\n",
    "\n",
    "    Returns:\n",
    "        loss: Tensor (scalar if reduction != \"none\")\n",
    "\n",
    "    ---\n",
    "\n",
    "    From the tossing bot paper: \"We pass gradients only through the single pixel i on which the grasping primitive was executed. All other pixels backpropagate with 0 loss.\"\n",
    "\n",
    "    ##### Key ideas to implement:\n",
    "    - Instead of applying BCE loss over the entire (C, H, W) tensor, select logits at the executed (c, h, w) indices.\n",
    "    - This ensures the computational graph only flows through those selected entries, so gradients elsewhere are zero.\n",
    "    - To handle multiple executed actions per sample, gather all relevant indices, compute the loss for each, and average.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.info: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        executed_list: List[torch.Tensor],\n",
    "        labels_list: List[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        B, C, H, W = logits.shape\n",
    "        device = logits.device\n",
    "\n",
    "        \"\"\" \n",
    "            Currently lists look something like:\n",
    "\n",
    "            executed_list = [\n",
    "                [[3, 20, 15], [7, 40, 10]],    # B0 x 2\n",
    "                [[7, 40, 10]],                 # B1 x 1\n",
    "                [],                            # B2 x 0 (no action)\n",
    "                [[5, 50, 50], [5, 51, 50], [5, 52, 50]],  # B3 x 3\n",
    "            ]\n",
    "            labels_list = [\n",
    "                [1, 0],\n",
    "                [1],\n",
    "                [],\n",
    "                [0, 0, 1],\n",
    "            ]\n",
    "\n",
    "            We want to convert into a single list to easily index the logics directly with a single look up.\n",
    "\n",
    "            b_all = [0, 0, 1, 3, 3, 3] # Notice how batch 2 disappears..\n",
    "            c_all = [3, 7, 7, 5, 5, 5]\n",
    "            h_all = [20, 40, 40, 50, 51, 52]\n",
    "            w_all = [15, 10, 10, 50, 50, 50]\n",
    "            y_all = [1, 0, 1, 0, 0, 1]\n",
    "\n",
    "        \"\"\"\n",
    "        # Flatten variable-K actions across the batch\n",
    "        b_all, c_all, h_all, w_all, y_all = [], [], [], [], []\n",
    "        for i, (inds_i, y_i) in enumerate(zip(executed_list, labels_list)):\n",
    "            if inds_i is None or len(inds_i) == 0:\n",
    "                continue\n",
    "            inds_i = inds_i.to(device)       # [Ki, 3] (c,h,w)\n",
    "            y_i = y_i.to(device).float()     # [Ki]    (0/1)\n",
    "            Ki = inds_i.shape[0]\n",
    "\n",
    "            b_all.append(torch.full((Ki,), i, device=device, dtype=torch.long))\n",
    "            c_all.append(inds_i[:, 0].long())\n",
    "            h_all.append(inds_i[:, 1].long())\n",
    "            w_all.append(inds_i[:, 2].long())\n",
    "            y_all.append(y_i)\n",
    "\n",
    "        # If no actions executed in the whole batch, return zero (still connected)\n",
    "        if len(b_all) == 0:\n",
    "            zero = logits.new_tensor(0.0, requires_grad=True)\n",
    "            self.info = {\"empty\": torch.tensor(True, device=device)}\n",
    "            return zero\n",
    "\n",
    "        b_all = torch.cat(b_all)  # [N]\n",
    "        c_all = torch.cat(c_all)  # [N]\n",
    "        h_all = torch.cat(h_all)  # [N]\n",
    "        w_all = torch.cat(w_all)  # [N]\n",
    "        y_all = torch.cat(y_all)  # [N]\n",
    "\n",
    "        # Index only the executed logits -> gradients flow only here\n",
    "        selected_logits = logits[b_all, c_all, h_all, w_all]  # [N]\n",
    "\n",
    "        # Standard BCEWithLogitsLoss reduction\n",
    "        loss = super().forward(selected_logits, y_all)\n",
    "\n",
    "        # Save debug info for later inspection\n",
    "        self.info = {\n",
    "            \"b_all\": b_all.detach(),\n",
    "            \"c_all\": c_all.detach(),\n",
    "            \"h_all\": h_all.detach(),\n",
    "            \"w_all\": w_all.detach(),\n",
    "            \"y_all\": y_all.detach(),\n",
    "        }\n",
    "\n",
    "        return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    B, C, H, W = 4, 12, 64, 64\n",
    "    logits = torch.randn(B, C, H, W, requires_grad=True)\n",
    "\n",
    "    executed_list = [\n",
    "        torch.tensor([[3,20,15],[7,40,10]]),   # K0=2\n",
    "        torch.tensor([[7,40,10]]),             # K1=1\n",
    "        torch.tensor([], dtype=torch.long).reshape(0,3),  # K2=0\n",
    "        torch.tensor([[5,50,50],[5,51,50],[5,52,50]]),    # K3=3\n",
    "    ]\n",
    "    labels_list = [\n",
    "        torch.tensor([1, 0], dtype=torch.float32),\n",
    "        torch.tensor([1], dtype=torch.float32),\n",
    "        torch.tensor([], dtype=torch.float32),\n",
    "        torch.tensor([0, 0, 1], dtype=torch.float32),\n",
    "    ]\n",
    "\n",
    "    loss_fn = MaskedBCEWithLogitsLoss(reduction=\"mean\")\n",
    "    loss = loss_fn(logits, executed_list, labels_list)\n",
    "    loss.backward()\n",
    "\n",
    "    print(\"loss:\", float(loss))\n",
    "    print(\"info dict:\")\n",
    "\n",
    "    expected_values = dict()\n",
    "    expected_values['b_all'] = torch.tensor([0, 0, 1, 3, 3, 3])\n",
    "    expected_values['c_all'] = torch.tensor([3, 7, 7, 5, 5, 5])\n",
    "    expected_values['h_all'] = torch.tensor([20, 40, 40, 50, 51, 52])\n",
    "    expected_values['w_all'] = torch.tensor([15, 10, 10, 50, 50, 50])\n",
    "    expected_values['y_all'] = torch.tensor([1, 0, 1, 0, 0, 1])\n",
    "\n",
    "    for k, v in loss_fn.info.items():\n",
    "        print(k, v)\n",
    "        assert torch.all(value == expected_values[key]), f\"Expected {key} to be {expected_values[key]}, but got {value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
