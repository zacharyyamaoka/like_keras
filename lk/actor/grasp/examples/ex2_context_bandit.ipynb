{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I also just use the tabular approach? likely...\n",
    "\n",
    "The big difference here, is that now the state changes. The right action to take depends on what state you are in!\n",
    "\n",
    "Its like you are walking through various rooms in the casino. For now we can assume that its a one step problem, and then next state is random.\n",
    "\n",
    "Why doesn't the tabular case end up working in reality?....\n",
    "\n",
    "Well for the action space... I don't think you can do much better actually than a average reward + epsiolon greedy exploration/explotation strategy.\n",
    "\n",
    "\n",
    "Ok here is a fantastic post: https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/#summary\n",
    "\n",
    "Some options:\n",
    "1. No Exploration: Greedy algorithim, we always do what we think is best\n",
    "2. Random Exploration: E greedy algorithim, we randomly try new actions\n",
    "3. Smart exploration: Upper Confidence bounds (UCB), we select actions to try that we are uncertain about.\n",
    "\n",
    "For multi-step problems definetly a lot more complicated actually. I really like building the intutions from the ground up though.\n",
    "\n",
    "\n",
    "How many states???\n",
    "\n",
    "lets say we have 10 states. ok then I can still just solve with tabular. I like the idea of the state being a QR code... :) its like the first step towards an image. It can still just be a key look up... 100 states... \n",
    "\n",
    "Next is than a grey scale image, something mabye like mnist? \n",
    "\n",
    "Ok lets start with just a simple 10x10 binary image. How many states are there? 10x10 = 100, 2^100 ~ 10^30 states... LOL... ok \n",
    "\n",
    "Ok I will use the same approach. I will flatten the image, and then turn it into a key for a dictionary. that will access the table, and then increment that table... same thing,\n",
    "I can display the various bar charts, randomly select some. Its just going to take longer to fill in all the values.\n",
    "\n",
    "---\n",
    "The only reason you are able to do better is that there is structure.\n",
    "If each state is truley random, then we are hoped...\n",
    "- If you change the color of the cup, the same actions should still apply\n",
    "- Wether its light or dark, the same actions should still apply, etc. etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import bam_gym # import to that __init__.py is run to register envs\n",
    "from bam_gym import print_reset, print_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Square numbers are easy to see as square\n",
    "n_arms = 5\n",
    "n_states = 10\n",
    "env = gym.make(\"bam_local/ContextBandit\", render_mode=\"human\", clip=10, n_arms=n_arms, n_states=n_states)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Reset:\", env.reset())\n",
    "\n",
    "env.unwrapped.render(sample_states=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "print_reset(state, info)\n",
    "\n",
    "def reset_env(env, terminated, truncated, info) -> bool:\n",
    "    if terminated or truncated:\n",
    "        if env.metadata['autoreset_mode'] != gym.vector.AutoresetMode.SAME_STEP:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    # action = policy(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print_step(action, state, reward, terminated, truncated, info, i)\n",
    "\n",
    "    if reset_env(env, terminated, truncated, info):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use a super duper simple policy to beat this, essentially just average the rewards to date, and then pick the highest action with epislon greedy probability\n",
    "# If you want to get a very accuarte understanding of the world, then you can continue to do random exploration, but to actually maximise reward you want to exploit as well!\n",
    "\n",
    "class StateAveragePolicy():\n",
    "    def __init__(self, n_actions, epsilon=0.2):\n",
    "\n",
    "        self.state_action_table = {}\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.iter = 0\n",
    "\n",
    "\n",
    "    def make_new_action_table(self, state):\n",
    "        self.state_action_table[state] = dict()\n",
    "        self.state_action_table[state]['attempts'] = np.ones(self.n_actions)\n",
    "        self.state_action_table[state]['reward'] = np.ones(self.n_actions)/self.n_actions\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, state):\n",
    "        self.iter += 1\n",
    "\n",
    "        if state not in self.state_action_table:\n",
    "            self.make_new_action_table(state)\n",
    "\n",
    "        action_values = self.state_action_table[state]['reward'] / self.state_action_table[state]['attempts']\n",
    "\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(action_values)\n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        if state not in self.state_action_table:\n",
    "            self.make_new_action_table(state)\n",
    "\n",
    "        self.state_action_table[state]['attempts'][action] += 1\n",
    "        self.state_action_table[state]['reward'][action] += reward\n",
    "    \n",
    "    @property\n",
    "    def W(self):\n",
    "        W = dict()\n",
    "        for state in self.state_action_table:\n",
    "            W[state] = self.state_action_table[state]['reward'] / self.state_action_table[state]['attempts']\n",
    "        return W\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "policy = StateAveragePolicy(n_actions=n_arms, epsilon=0.2)\n",
    "print(policy.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "policy.epsilon = 0.0 # Manually tune epsilon \n",
    "\n",
    "for i in range(1000):\n",
    "    action = policy(state)\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(new_state)\n",
    "    # print_step(action, state, reward, terminated, truncated, info, i)\n",
    "    policy.update(state, action, reward)\n",
    "    reward_history.append(reward)\n",
    "    epsilon_history.append(policy.epsilon)\n",
    "    state = new_state\n",
    "\n",
    "    if reset_env(env, terminated, truncated, info):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "print(policy.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for s in range(max(n_states, 5)):\n",
    "    width = 0.4\n",
    "    x = np.arange(n_arms)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.bar(x - width/2, env.unwrapped.arm_thresholds[s], width, alpha=0.7, label=\"Actual\")\n",
    "    plt.bar(x + width/2, policy.W[s], width, alpha=0.7, label=\"Estimated\")\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.xlabel('Arm')\n",
    "    plt.ylabel('Threshold')\n",
    "    plt.title(f'State {s}: Thresholds')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history, label=\"Reward\")\n",
    "plt.plot(epsilon_history, label=\"Epsilon\", color='red')\n",
    "# Compute moving average\n",
    "window_size = 50\n",
    "if len(reward_history) >= window_size:\n",
    "    moving_avg = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(reward_history)), moving_avg, label=f\"Moving Avg ({window_size})\", color='orange')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward History and Moving Average\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
