{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- In the spirit of RGB matters, I am not going to predict the width...\n",
    "- that could be done with FAS, and is depdent on cordinates, whilest the orientation and position is not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "import gymnasium as gym\n",
    "import bam_gym \n",
    "from bam_gym import print_reset, print_step\n",
    "import time\n",
    "np.random.seed(1)\n",
    "from bam_artist.heatmap_helper import show_heatmap_img, show_heatmap_img_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_size = 2**5\n",
    "min_width = np.ceil(screen_size*0.01)\n",
    "max_width = min_width * 3\n",
    "print(\"Screen Size:\", screen_size)\n",
    "print(\"Min Width:\", min_width)\n",
    "print(\"Max Width:\", max_width)\n",
    "render_screen_scale = 500/screen_size\n",
    "N_ANGLES = 6    \n",
    "ANGLE_RANGE = [0, np.pi]\n",
    "\n",
    "env = gym.make(\n",
    "    \"bam_local/GraspXYRZ\",\n",
    "    render_mode=\"human\",\n",
    "    screen_size=(screen_size, screen_size),\n",
    "    num_rectangles=5,\n",
    "    min_width=min_width,\n",
    "    max_width=max_width,\n",
    "    min_height_ratio=2,\n",
    "    max_height_ratio=5.0,\n",
    "    render_screen_scale=render_screen_scale,\n",
    "    n_angles=N_ANGLES,\n",
    "    angle_range=ANGLE_RANGE\n",
    ")\n",
    "\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "state, info = env.reset()\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_labels(state, label, alpha=0.5):\n",
    "    labels = []\n",
    "\n",
    "    for i in range(N_ANGLES):\n",
    "        step_size = (ANGLE_RANGE[1] - ANGLE_RANGE[0]) / N_ANGLES\n",
    "        angle = ANGLE_RANGE[0] + i*step_size\n",
    "        # Wrap angle from pi to -pi, for easier understanding...\n",
    "        wrapped_angle = (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "        labels.append(f\"Angle {np.rad2deg(wrapped_angle):.1f}째 ~ {wrapped_angle:.1f}rad\")\n",
    "        # labels.append(f\"Angle {np.rad2deg(angle):.1f}째 ~ {angle:.1f}rad\")\n",
    "\n",
    "    # When thinking about if the angle is correct, keep in mind the neutral positoin of the gripper... if at gripper 0, it should pick up an item along the x axis...\n",
    "\n",
    "    show_heatmap_img_grid(state, label, rows=2, cols=8, alpha=alpha, title_list=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "env.unwrapped.render_mode = \"none\"\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    print(action)\n",
    "    # action = policy(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    view_labels(state, info[\"curr_label\"], alpha=0.9)\n",
    "    # time.sleep(0.5)\n",
    "    # print_step(action, state, reward, terminated, truncated, info, i)\n",
    "    break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.shape)\n",
    "label = env.unwrapped.get_binary_label()\n",
    "print(label.shape)\n",
    "\n",
    "# for row in np.argmax(label, axis=2):\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scale = 100\n",
    "train_dataset_size = 6 * dataset_scale\n",
    "test_dataset_size = 1 * dataset_scale\n",
    "\n",
    "\n",
    "\n",
    "def to_tensor_pairs(numpy_pairs: list[tuple[np.ndarray, np.ndarray]]):\n",
    "    tensor_pairs = []\n",
    "    for img, mask in numpy_pairs:\n",
    "        img_t = torch.from_numpy(img).permute(2,0,1).float() / 255.0\n",
    "        mask_t = torch.from_numpy(mask).permute(2,0,1).float()\n",
    "        tensor_pairs.append((img_t, mask_t))\n",
    "    return tensor_pairs\n",
    "\n",
    "train_data_pairs = to_tensor_pairs(env.unwrapped.create_dataset(train_dataset_size))\n",
    "\n",
    "test_data_pairs = to_tensor_pairs(env.unwrapped.create_dataset(test_dataset_size))\n",
    "\n",
    "\n",
    "print(\"Size of train tensor dataset:\", len(train_data_pairs))\n",
    "print(\"Shape of first image:\", train_data_pairs[0][0].shape)\n",
    "print(\"Shape of first mask:\", train_data_pairs[0][1].shape)\n",
    "\n",
    "print(\"Size of test tensor dataset:\", len(test_data_pairs))\n",
    "print(\"Shape of first image:\", test_data_pairs[0][0].shape)\n",
    "print(\"Shape of first mask:\", test_data_pairs[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_plot = []\n",
    "for i in range(64):\n",
    "    images_to_plot.append(train_data_pairs[i][0])\n",
    "\n",
    "images_to_plot = torch.stack(images_to_plot)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "ax.imshow(make_grid(images_to_plot[:64], nrow=8, pad_value=1).permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Nvidia Failure, solution was just to restart...\n",
    "import torch, subprocess, os\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "print(\"device_count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"current_device:\", torch.cuda.current_device())\n",
    "    print(\"name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"capability:\", torch.cuda.get_device_capability(0))\n",
    "    try:\n",
    "        a = torch.randn(1, device=\"cuda\")\n",
    "        b = a.cpu()\n",
    "        print(\"basic CUDA op: OK\")\n",
    "        free, total = torch.cuda.mem_get_info()\n",
    "        print(\"mem free/total (MB):\", free//2**20, \"/\", total//2**20)\n",
    "    except Exception as e:\n",
    "        print(\"basic CUDA op failed:\", e)\n",
    "\n",
    "try:\n",
    "    print(subprocess.check_output([\"nvidia-smi\"]).decode().splitlines()[0])\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in train_data_pairs], dim=0)\n",
    "\n",
    "# Compute per-channel mean and std across N,H,W\n",
    "mean = imgs.mean(dim=(0,2,3))   # [3]\n",
    "std  = imgs.std(dim=(0,2,3))    # [3]\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HeatmapDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_label_pairs: list[tuple[torch.Tensor, torch.Tensor]],\n",
    "        transform: Callable | None = None,\n",
    "        label_transforms: Callable | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        tensor_pairs: list of (img, mask) where\n",
    "            img: torch.Tensor [3,H,W], float in [0,1]\n",
    "            mask: torch.Tensor [1,H,W], float {0,1}\n",
    "        transform: applied to images\n",
    "        label_transforms: applied to masks\n",
    "        \"\"\"\n",
    "        self.data = input_label_pairs\n",
    "        self.transform = transform\n",
    "        self.label_transforms = label_transforms\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.label_transforms:\n",
    "            label = self.label_transforms(label)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std, inplace=True),\n",
    "])\n",
    "label_transforms = None\n",
    "# transforms.Compose([\n",
    "#     # transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean, std, inplace=True),\n",
    "# ])\n",
    "\n",
    "\n",
    "batch_size = 2**9\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "\n",
    "train_dataset = HeatmapDataset(train_data_pairs, transform=transform, label_transforms=label_transforms)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_dataset = HeatmapDataset(test_data_pairs, transform=transform, label_transforms=label_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(data_loader: DataLoader):\n",
    "    for images, labels in data_loader:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8, pad_value=1).permute(1, 2, 0))\n",
    "        break\n",
    "\n",
    "show_batch(train_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_batch_histograms(data_loader: torch.utils.data.DataLoader, n_batches: int = 1):\n",
    "    \"\"\"\n",
    "    Show histograms of pixel values from a few batches.\n",
    "    - Plots per-channel histograms for images\n",
    "    - Plots mask pixel distribution\n",
    "    \"\"\"\n",
    "    for batch_idx, (images, masks) in enumerate(data_loader):\n",
    "        # Flatten image values: [B, C, H, W] -> [Npixels_per_channel]\n",
    "        img_vals = images.permute(1,0,2,3).reshape(images.shape[1], -1)  # [C, N]\n",
    "        \n",
    "        # Flatten mask values: [B,1,H,W] -> [N]\n",
    "        mask_vals = masks.reshape(-1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # --- Image histograms (per channel) ---\n",
    "        colors = ['r', 'g', 'b'] if images.shape[1] == 3 else ['k']\n",
    "        for c in range(images.shape[1]):\n",
    "            axes[0].hist(img_vals[c].numpy().ravel(), bins=50, color=colors[c], alpha=0.5, label=f\"Channel {c}\")\n",
    "        axes[0].set_title(\"Image Pixel Distribution\")\n",
    "        axes[0].legend()\n",
    "\n",
    "        # --- Mask histogram ---\n",
    "        axes[1].hist(mask_vals.numpy().ravel(), bins=50, color='gray', alpha=0.7)\n",
    "        axes[1].set_title(\"Mask Pixel Distribution\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        if batch_idx + 1 >= n_batches:\n",
    "            break\n",
    "show_batch_histograms(train_dataloader, n_batches=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bam_grasp.net.layers.Heatmap_Resnet import HeatmapResNet\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HeatmapResNet(num_layers=18, out_channels=N_ANGLES).to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model, \n",
    "    input_size=(batch_size, 3, 32, 32),  # Example input size for CIFAR images\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one batch\n",
    "img, label = next(iter(train_dataloader))\n",
    "img, label = img.to(device), label.to(device)\n",
    "print(img.shape, label.shape)\n",
    "logits = model(img) \n",
    "print(logits.shape)\n",
    "\n",
    "assert logits.shape == label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "epoch_total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader: DataLoader) -> dict[str, float]:\n",
    "    model.train()\n",
    "    n_total = len(data_loader.dataset)\n",
    "    n_batches = len(data_loader)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X) \n",
    "        resized_pred = nn.functional.interpolate(pred, size=y.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "        train_loss: torch.Tensor = loss_fn(resized_pred, y) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = train_loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{n_total:>5d}]\")\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    return {\"avg_loss\": avg_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader: DataLoader) -> dict[str, float]:\n",
    "    n_total = len(data_loader.dataset)\n",
    "    n_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X: torch.Tensor = X.to(device)\n",
    "            y: torch.Tensor = y.to(device)\n",
    "            logits: torch.Tensor = model(X)\n",
    "\n",
    "            # accumulate loss\n",
    "            test_loss += loss_fn(logits, y).item()\n",
    "\n",
    "            # BUG: you cannot use this classificaiton style accuracy\n",
    "            # correct += (logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # --- accuracy ---\n",
    "            N, C, H, W = logits.shape\n",
    "            # flatten spatial+channel dims\n",
    "            logits_flat = logits.view(N, -1)   # (N, C*H*W)\n",
    "            y_flat = y.view(N, -1)             # (N, H*W) or (N, C*H*W) depending on encoding\n",
    "\n",
    "            # argmax per sample\n",
    "            pred_idx = logits_flat.argmax(dim=1)  # (N,)\n",
    "\n",
    "            hits = y_flat[torch.arange(N), pred_idx]          # (N,)\n",
    "            correct += (hits == 1).sum().item()\n",
    "            total += N\n",
    "\n",
    "\n",
    "    avg_loss = test_loss / n_batches\n",
    "    accuracy = correct / n_total\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return {\"test_loss\": test_loss, \"test_accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_total += 1\n",
    "    print(f\"Epoch {epoch_total}\\n-------------------------------\")\n",
    "    train_info = train(train_dataloader)\n",
    "    test_info = test(test_dataloader)\n",
    "    # test_info = test(train_dataloader) # can I overfit to 100% a dataset? \n",
    "\n",
    "    train_losses.append(train_info[\"avg_loss\"])\n",
    "    test_losses.append(test_info[\"test_loss\"])\n",
    "    test_accuracies.append(test_info[\"test_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_state(state: np.ndarray):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.RandomCrop(32, padding=4),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std, inplace=False),\n",
    "    ])\n",
    "\n",
    "    state_tensor = torch.from_numpy(state).permute(2,0,1).float() / 255.0 # Convert to tensor as above\n",
    "    state_tensor = transform(state_tensor) # Apply same transforms as test dataset\n",
    "    state_tensor = state_tensor.unsqueeze(0) # add batch dim\n",
    "    state_tensor = state_tensor.to(device) # move to same device as model\n",
    "\n",
    "    return state_tensor\n",
    "\n",
    "# Edited on Sept 4 to change order.. of x, y, c. May to need play around if it doens't work any more...\n",
    "class HeatmapPolicy():\n",
    "    def __init__(self, model: nn.Module, preprocess_state: Callable, epsilon=0.2):\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.epsilon = epsilon\n",
    "        self.iter = 0\n",
    "        self.preprocess_state = preprocess_state\n",
    "        self.last_heatmaps = None\n",
    "\n",
    "\n",
    "    def __call__(self, state: np.ndarray):\n",
    "\n",
    "        # Be very careful with order of output actions.\n",
    "        # Gym usally expects x, y, c, Pytorch gives c, x, y\n",
    "        self.iter += 1\n",
    "\n",
    "        state = self.preprocess_state(state)\n",
    "\n",
    "\n",
    "        heatmaps = self.model(state)\n",
    "        # print(\"Logits shape:\", heatmaps.shape)\n",
    "        self.last_heatmaps = heatmaps\n",
    "        B, C, H, W = heatmaps.shape\n",
    "\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            cs = torch.randint(C, (B,)) \n",
    "            xs = torch.randint(W, (B,))\n",
    "            ys = torch.randint(H, (B,))\n",
    "\n",
    "            coords = torch.stack([xs, ys, cs], dim=1)  # [B,3]\n",
    "            if C == 1:\n",
    "                coords = coords[:, :2]  # drop channel column -> [B,2]\n",
    "\n",
    "            if B == 1:\n",
    "                return tuple(int(v) for v in coords[0])\n",
    "            return coords\n",
    "\n",
    "        # Exploit: global argmax over (C,H,W) for each sample\n",
    "        heatmaps_flat = heatmaps.view(B, -1)                     # [B, C*H*W]\n",
    "        argmax_idx = heatmaps_flat.argmax(dim=1)               # [B]\n",
    "\n",
    "        # Decode to (x, y, c), then output as (x, y, c) (or (x,y) if C==1)\n",
    "        c = argmax_idx // (H * W)                     # [B]\n",
    "        rem = argmax_idx % (H * W)\n",
    "        y = rem // W\n",
    "        x = rem % W\n",
    "\n",
    "        if C == 1:\n",
    "            if B == 1:\n",
    "                return int(x.item()), int(y.item())\n",
    "            return torch.stack([x, y], dim=1)         # [B,2] -> (x,y)\n",
    "        else:\n",
    "            if B == 1:\n",
    "                return int(c.item()), int(x.item()), int(y.item())\n",
    "            return torch.stack([x, y, c], dim=1)      # [B,3] -> (x,y,c)\n",
    "\n",
    "print(preprocess_state(state).shape)\n",
    "\n",
    "policy = HeatmapPolicy(model, preprocess_state, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "policy.epsilon = 0.0\n",
    "action = policy(state)\n",
    "print(action)\n",
    "\n",
    "# at zero deg the gripper should be point upwards! at 90, its 90deg offset..\n",
    "# Without the offset, then we are expecting the angle to be along the longital axis of the item\n",
    "gripper_offset = np.pi/2\n",
    "\n",
    "def display_heatmap(state, action, policy):\n",
    "    # Determine if action includes angle\n",
    "    if len(action) == 2:\n",
    "        x, y = action\n",
    "        angle = None\n",
    "    else:\n",
    "        c, x, y = action\n",
    "        angle = c * 2 * np.pi / N_ANGLES\n",
    "        angle *= -1 # make z go in other direction, I was expecting angle to start and x and go up, but... computer coordinates are strange...\n",
    "        # angle += gripper_offset\n",
    "        # angle = 0\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot state with action\n",
    "    axs[0].set_title(\"State\")\n",
    "    axs[0].imshow(state)\n",
    "    axs[0].scatter([x], [y], color='white', marker='x', s=100, label='Action')\n",
    "    if angle is not None:\n",
    "        dx = np.cos(angle) * max_width\n",
    "        dy = np.sin(angle) * max_width\n",
    "        axs[0].arrow(x, y, dx, dy, head_width=np.ceil(max_width*0.1), head_length=np.ceil(max_width*0.1), fc='white', ec='white')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot heatmap with action\n",
    "    axs[1].set_title(\"Heatmap\")\n",
    "    heatmap = policy.last_heatmaps[0,:,:,:].detach().cpu().numpy()\n",
    "    # BUG: Arrow was appearing on a low score place... its beacuse you where just plotting the top angle...\n",
    "    heatmap = heatmap.max(axis=0) # show the max of any of the angles\n",
    "    im = axs[1].imshow(heatmap, cmap='hot')\n",
    "    axs[1].scatter([x], [y], color='blue', marker='x', s=100, label='Action')\n",
    "    if angle is not None:\n",
    "        dx = np.cos(angle) * max_width\n",
    "        dy = np.sin(angle) * max_width\n",
    "        axs[1].arrow(x, y, dx, dy, head_width=np.ceil(max_width*0.1), head_length=np.ceil(max_width*0.1), fc='blue', ec='blue')\n",
    "    axs[1].legend()\n",
    "    fig.colorbar(im, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "C, X, Y = action\n",
    "angle = C*2*np.pi/N_ANGLES\n",
    "angle = (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "display_heatmap(state, action, policy)\n",
    "heatmap = policy.last_heatmaps[0,:,:,:].detach().cpu().numpy()\n",
    "heatmap = np.transpose(heatmap, (1, 2, 0))\n",
    "\n",
    "\n",
    "print(f\"Action Angle {np.rad2deg(angle):.1f}째 or {angle:.1f}rad\")\n",
    "\n",
    "view_labels(state, heatmap, alpha=0.9)\n",
    "view_labels(state, info[\"curr_label\"], alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.render_mode = \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "policy.epsilon = 0.0\n",
    "\n",
    "for i in range(10):\n",
    "    # action = env.action_space.sample()\n",
    "    action = policy(state)\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    reward_history.append(reward)\n",
    "    epsilon_history.append(policy.epsilon)\n",
    "\n",
    "    if reward == 0 and False:\n",
    "        C, X, Y = action\n",
    "        angle = C*2*np.pi/N_ANGLES\n",
    "        angle = (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "        display_heatmap(state, action, policy)\n",
    "        heatmap = policy.last_heatmaps[0,:,:,:].detach().cpu().numpy()\n",
    "        heatmap = np.transpose(heatmap, (1, 2, 0))\n",
    "\n",
    "     \n",
    "        print(f\"Action Angle {np.rad2deg(angle):.1f}째 or {angle:.1f}rad\")\n",
    "\n",
    "        view_labels(state, heatmap, alpha=0.9)\n",
    "        view_labels(state, info[\"last_label\"], alpha=0.9)\n",
    "\n",
    "        break\n",
    "    state = new_state\n",
    "\n",
    "    # print_step(action, state, reward, terminated, truncated, info, i)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history, label=\"Reward\")\n",
    "plt.plot(epsilon_history, label=\"Epsilon\", color='red')\n",
    "# Compute moving average\n",
    "window_size = 50\n",
    "if len(reward_history) >= window_size:\n",
    "    moving_avg = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(reward_history)), moving_avg, label=f\"Moving Avg ({window_size})\", color='orange')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward History and Moving Average\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
