{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now I feel ready to take on training on the grasp net dataset.\n",
    "\n",
    "- This is still just grasp xy rx ry rz beacuse we are not doing the FAS for z width yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import wandb\n",
    "from typing import Callable\n",
    "import gymnasium as gym\n",
    "import yaml\n",
    "\n",
    "# BAM\n",
    "import bam_gym \n",
    "from bam_gym import print_reset, print_step\n",
    "from bam_artist.heatmap_helper import show_heatmap_img, show_heatmap_img_grid\n",
    "from bam_grasp.net.layers import HeatmapResNet, MaskedBCEWithLogitsLoss\n",
    "\n",
    "# TORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgb_matters.constant import GRASPNET_ROOT, LABEL_DIR\n",
    "from rgb_matters.data.utils.generate_anchor_matrix import NUM_VIEWS, NUM_ANGLES\n",
    "from rgb_matters.data import GraspNetDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/bam/bam_ws/src/bam_brain/actor/examples/ex10_train_config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    C = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print(C)\n",
    "\n",
    "C[\"train_label_root\"] = LABEL_DIR\n",
    "C[\"test_label_root\"] = LABEL_DIR\n",
    "C[\"eval_for_n_batch\"] = 50\n",
    "C[\"num_layers\"] = 50\n",
    "\n",
    "C[\"epsilon\"] = 0.5\n",
    "C[\"mask_loss_fn\"] = False\n",
    "C[\"n_executed_actions\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    entity=\"zach-yamaoka-independent\",\n",
    "    project=\"bam-grasp\",\n",
    "    # job_type=\"test\",\n",
    "    group=\"ex10-grasp-xy-rxryrz\",\n",
    "    config=C\n",
    ")\n",
    "\n",
    "# max_action = 32 * 32 * 1\n",
    "# print(f\"Max action: {max_action}\")\n",
    "# assert run.config[\"n_executed_actions\"] <= max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = HeatmapResNet(num_layers=C[\"num_layers\"], out_channels=NUM_ANGLES*NUM_VIEWS, upsample_logits=False, input_config='none').to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "LR = float(C[\"lr\"])\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=LR)\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "masked_loss_fn = MaskedBCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([3, 288, 384]) torch.Size([1, 288, 384]) torch.Size([360, 72, 96]) torch.Size([0])\n",
    "# torch.Size([1, 6, 288, 384])\n",
    "info = summary(\n",
    "    model, \n",
    "    input_size=(1, 3, 288, 384),  # Example input size for CIFAR images\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\"]\n",
    ")\n",
    "print(info)\n",
    "torch.cuda.empty_cache()   # clears cached allocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgb_matters.net.rgb_normal_net import RGBNormalNet\n",
    "\n",
    "if False:\n",
    "\n",
    "    net = RGBNormalNet(\n",
    "        num_layers=50, use_normal=False, normal_only=False\n",
    "    )\n",
    "    info = summary(\n",
    "        net, \n",
    "        input_size=[(1, 3, 288, 384), (1, 3, 288, 384)],  # Example input size for CIFAR images\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\"]\n",
    "    )\n",
    "    print(info)\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graspnet_train = GraspNetDataset(\n",
    "    graspnet_root=GRASPNET_ROOT,\n",
    "    label_root=C[\"train_label_root\"],\n",
    "    use_normal=C[\"use_normal\"],\n",
    "    camera=C[\"train_camera\"],\n",
    "    split=C[\"split\"][\"train_split\"],\n",
    "    grayscale=C[\"augmentation\"][\"grayscale\"],\n",
    "    colorjitter_scale=C[\"augmentation\"][\"colorjitter_scale\"],\n",
    "    random_crop=C[\"augmentation\"][\"random_crop\"],\n",
    ")\n",
    "\n",
    "graspnet_test = GraspNetDataset(\n",
    "    graspnet_root=GRASPNET_ROOT,\n",
    "    label_root=C[\"test_label_root\"],\n",
    "    use_normal=C[\"use_normal\"],\n",
    "    camera=C[\"test_camera\"],\n",
    "    split=C[\"split\"][\"test_split\"],\n",
    "    grayscale=C[\"augmentation\"][\"grayscale\"],\n",
    "    colorjitter_scale=0, # no aug on testing\n",
    "    random_crop=0,\n",
    ")\n",
    "\n",
    "subset_indices = list(range(1))\n",
    "overfit_dataset = Subset(graspnet_train, subset_indices)\n",
    "\n",
    "eval_test_dataloader = DataLoader(\n",
    "    graspnet_test,\n",
    "    shuffle=True,\n",
    "    batch_size=C[\"eval_batch_size\"],\n",
    "    num_workers=12,\n",
    ")\n",
    "eval_train_dataloader = DataLoader(\n",
    "    graspnet_train,\n",
    "    shuffle=False,\n",
    "    batch_size=C[\"eval_batch_size\"],\n",
    "    num_workers=12,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    graspnet_train,\n",
    "    shuffle=True,\n",
    "    batch_size=C[\"batch_size\"],\n",
    "    num_workers=12,\n",
    ")\n",
    "\n",
    "overfit_dataloader = DataLoader(\n",
    "    overfit_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=C[\"batch_size\"],\n",
    "    num_workers=12,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bam_utils.python.torch_helper import show_batch, show_batch_histograms\n",
    "\n",
    "# show_batch(graspnet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one batch\n",
    "rgb, depth, label, normal = next(iter(graspnet_train))\n",
    "print(rgb.shape, depth.shape, label.shape, normal.shape)\n",
    "rgb, label = rgb.to(device), label.to(device)\n",
    "rgb = rgb.unsqueeze(0)\n",
    "\n",
    "logits = model(rgb) \n",
    "print(logits.shape)\n",
    "\n",
    "assert logits.shape[1:] == label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG = dict(\n",
    "    epoch_total = 0,\n",
    "    batch_total = 0,\n",
    "    samples_total = 0,\n",
    "    wall_time_total = 0,\n",
    "    lr = float(C[\"lr\"])\n",
    ")\n",
    "\n",
    "print(LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader: DataLoader):\n",
    "    model.train()\n",
    "    n_total = len(data_loader.dataset)\n",
    "    n_batches = len(data_loader)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    K = C[\"n_executed_actions\"]\n",
    "\n",
    "    for batch, (rgb, depth, label, normal) in enumerate(data_loader):\n",
    "        X, y = rgb.to(device), label.to(device)\n",
    "\n",
    "        LOG[\"batch_total\"] += 1\n",
    "        LOG[\"samples_total\"] += len(X)\n",
    "\n",
    "        logits = model(X) \n",
    "\n",
    "        if C[\"mask_loss_fn\"]: # this essential does epsilon = 1.0 random selection... may be suboptimal beacuse you keep on learning about the wrong actions... grr...\n",
    "\n",
    "            B, CH, H, W = logits.shape\n",
    "            flat = logits.view(B, -1)                  # [B, C*H*W]\n",
    "            assert K <= flat.size(1)\n",
    "\n",
    "            # 1) Exploitation: per-batch top-K (largest logits)\n",
    "            top_vals, top_flat_idx = torch.topk(flat, K, dim=1)  # [B, K]\n",
    "            c = top_flat_idx // (H * W)\n",
    "            rem = top_flat_idx % (H * W)\n",
    "            h = rem // W\n",
    "            w = rem % W\n",
    "            executed_list = torch.stack([config_path, h, w], dim=2).to(torch.long)  # [B, K, 3]\n",
    "\n",
    "            # 2) Exploration: with prob epsilon, replace each selected (c,h,w) with a random one\n",
    "            if C[\"epsilon\"] > 0.0:\n",
    "                explore_mask = torch.bernoulli(\n",
    "                    torch.full((B, K), float(C[\"epsilon\"]), device=X.device)\n",
    "                ).bool()  # True => replace with random\n",
    "\n",
    "                if explore_mask.any():\n",
    "                    rand_c = torch.randint(0, CH, (B, K), device=X.device)\n",
    "                    rand_h = torch.randint(0, H, (B, K), device=X.device)\n",
    "                    rand_w = torch.randint(0, W, (B, K), device=X.device)\n",
    "\n",
    "                    executed_list[..., 0] = torch.where(explore_mask, rand_c, executed_list[..., 0])\n",
    "                    executed_list[..., 1] = torch.where(explore_mask, rand_h, executed_list[..., 1])\n",
    "                    executed_list[..., 2] = torch.where(explore_mask, rand_w, executed_list[..., 2])\n",
    "\n",
    "            assert executed_list.shape == (B, K, 3)\n",
    "\n",
    "            # Gather labels at executed indices: y is [B, C, H, W] -> labels [B, K]\n",
    "            b_idx = torch.arange(B, device=X.device)[:, None].expand(B, K)\n",
    "            labels = y[\n",
    "                b_idx,\n",
    "                executed_list[..., 0],\n",
    "                executed_list[..., 1],\n",
    "                executed_list[..., 2],\n",
    "            ]  # [B, K]\n",
    "\n",
    "            executed_list_per_batch = list(executed_list.unbind(0))  # B × [K,3]\n",
    "            labels_per_batch        = list(labels.unbind(0))         # B × [K]\n",
    "\n",
    "            train_loss: torch.Tensor = masked_loss_fn.forward(logits, executed_list_per_batch, labels_per_batch)\n",
    "        else:\n",
    "            train_loss: torch.Tensor = loss_fn(logits, y) \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        if C[\"grad_clip\"] > 0:\n",
    "            clip_grad_norm_(model.parameters(), C[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = train_loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{n_total:>5d}]\")\n",
    "\n",
    "            run.log({\n",
    "                \"train/train_loss\": loss,\n",
    "            },\n",
    "            step = LOG[\"samples_total\"])\n",
    "\n",
    "        if LOG[\"batch_total\"] % (20000) == 0 and LOG[\"batch_total\"] > 100:\n",
    "            LR *= float(C[\"lr_decay\"])\n",
    "            optimizer = torch.optim.Adam(params = model.parameters(), lr=LR)\n",
    "            LOG[\"lr\"] = LR\n",
    "\n",
    "\n",
    "    LOG[\"avg_train_loss\"] = running_loss / n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader: DataLoader, namespace=\"\") -> dict[str, float]:\n",
    "    n_total = len(data_loader.dataset)\n",
    "    n_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (rgb, depth, label, normal) in enumerate(data_loader):\n",
    "            X: torch.Tensor = rgb.to(device)\n",
    "            y: torch.Tensor = label.to(device)\n",
    "            logits: torch.Tensor = model(X)\n",
    "\n",
    "            # accumulate loss\n",
    "            test_loss += loss_fn(logits, y).item()\n",
    "\n",
    "            # BUG: you cannot use this classificaiton style accuracy\n",
    "            # correct += (logits.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "            # --- accuracy ---\n",
    "            N, CH, H, W = logits.shape\n",
    "            # flatten spatial+channel dims\n",
    "            logits_flat = logits.view(N, -1)   # (N, C*H*W)\n",
    "            y_flat = y.view(N, -1)             # (N, H*W) or (N, C*H*W) depending on encoding\n",
    "\n",
    "            # argmax per sample\n",
    "            pred_idx = logits_flat.argmax(dim=1)  # (N,)\n",
    "\n",
    "            hits = y_flat[torch.arange(N), pred_idx]          # (N,)\n",
    "            correct += (hits == 1).sum().item()\n",
    "            total += N\n",
    "\n",
    "            if i > C[\"eval_for_n_batch\"]:\n",
    "                break\n",
    "\n",
    "\n",
    "    LOG[namespace + \"avg_test_loss\"] = test_loss / n_batches\n",
    "    LOG[namespace + \"avg_test_accuracy\"] = correct / n_total\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*LOG[namespace + \"avg_test_accuracy\"]):>0.1f}%, Avg loss: {LOG[namespace + \"avg_test_loss\"]:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if C[\"mask_loss_fn\"]:\n",
    "    print(f\"Using masked loss fucntion for {C['n_executed_actions']} actions executed\")\n",
    "\n",
    "for epoch in range(300):\n",
    "    print(f\"Epoch {LOG['epoch_total']}\\n-------------------------------\")\n",
    "\n",
    "    epoch_start = time.time()\n",
    "    if True:\n",
    "        train(train_dataloader)\n",
    "        LOG[\"wall_time_total\"] += time.time() - epoch_start\n",
    "        LOG[\"epoch_total\"] += 1\n",
    "\n",
    "        test(eval_train_dataloader, 'eval_train/') \n",
    "        test(eval_test_dataloader, 'eval_test/') \n",
    "\n",
    "    if False:\n",
    "        train(overfit_dataloader)\n",
    "        LOG[\"wall_time_total\"] += time.time() - epoch_start\n",
    "        LOG[\"epoch_total\"] += 1\n",
    "\n",
    "        test(overfit_dataloader, 'eval_train/') \n",
    "        test(eval_test_dataloader, 'eval_test/') \n",
    "\n",
    "\n",
    "    run.log(LOG)\n",
    "\n",
    "    \n",
    "    # Checkpoint the model every 100 epochs\n",
    "    if LOG[\"epoch_total\"] % 100 == 0:\n",
    "        checkpoint_path = f\"model_checkpoint_epoch_{LOG['epoch_total']}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': LOG[\"epoch_total\"],\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr': LOG[\"lr\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO! Time to write this as a standalone python script!!!!\n",
    "with args... just like a launch file :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
