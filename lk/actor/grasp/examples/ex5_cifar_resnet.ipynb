{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://www.kaggle.com/code/kmldas/cifar10-resnet-90-accuracy-less-than-5-min\n",
    "\n",
    "And: https://github.com/GouMinghao/rgb_matters/blob/main/rgbd_graspnet/net/fastpose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchinfo import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data from disk and calculate mean and std\n",
    "DatasetType = Dataset[tuple[torch.Tensor, int]]\n",
    "\n",
    "CIFAR = \"CIFAR10\"   # or \"CIFAR100\"\n",
    "num_classes = 10 if CIFAR == \"CIFAR10\" else 100\n",
    "CIFARDataset = getattr(datasets, CIFAR)\n",
    "\n",
    "data_path = \"/home/bam/bam_ws/src/bam_brain/bam_gym/data_downloads\"\n",
    "\n",
    "# Load CIFAR without normalization so we can compute stats\n",
    "dataset: DatasetType = CIFARDataset(\n",
    "    root=data_path, train=True, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False, num_workers=4)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "n_samples = 0\n",
    "\n",
    "images: torch.Tensor\n",
    "for images, label in loader:\n",
    "    # images shape: [batch, channels, height, width]\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)  # [batch, C, H*W]\n",
    "    \n",
    "    mean += images.mean(2).sum(0)   # sum over batch\n",
    "    std += images.std(2).sum(0)\n",
    "    n_samples += batch_samples\n",
    "\n",
    "mean /= n_samples\n",
    "std /= n_samples\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data loaders with transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std, inplace=True),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std, inplace=True),\n",
    "])\n",
    "\n",
    "# Q: inplace true or false? https://chatgpt.com/c/68b5d239-5c54-8322-a73e-6f177e0545db\n",
    "# A: If you don't need to access the original tensor after normalization, set inplace=True for efficiency.\n",
    "\n",
    "train_dataset = CIFARDataset(root=data_path, train=True, download=True, transform=train_transforms)\n",
    "test_dataset  = CIFARDataset(root=data_path, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset,  batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_factory(num_layers=18, num_classes=10) -> models.resnet.ResNet:\n",
    "\n",
    "    \n",
    "\n",
    "    assert num_layers in [18, 34, 50, 101, 152]\n",
    "    model = eval(f\"models.resnet{num_layers}(pretrained=True)\") # pretrained is legacy interface, you can no specific the exact weights to load in\n",
    "\n",
    "    # Replace the 7x7 stride-2 conv + maxpool with 3x3 stride-1 and no pool:\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    # Adjust classifier head:\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_layers = 18\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet_factory(num_layers, num_classes).to(device)\n",
    "model_stats = summary(model, input_size=(100, 3, 32, 32))\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(data_loader: DataLoader):\n",
    "    for images, labels in data_loader:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CIFAR images (RGB)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))  \n",
    "\n",
    "for ax, i in zip(axes, np.random.randint(0, len(train_dataset), 3)):\n",
    "    image, label = train_dataset[i]\n",
    "    # image is a torch.Tensor in (C, H, W), unnormalize for display\n",
    "    img = image.clone()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]\n",
    "    img = img.numpy().transpose(1, 2, 0)  # (H, W, C)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    ax.set_title(f'Label: {train_dataset.classes[label]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Histogram of Pixel Values (Normalized)')\n",
    "# Show histogram for all channels\n",
    "img0 = train_dataset[0][0]\n",
    "\n",
    "plt.hist(img0.numpy().flatten(), bins=30, color='gray', alpha=0.7)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Histogram of Pixel Values (Raw)')\n",
    "\n",
    "for c in range(3):\n",
    "    img0[c] = img0[c] * std[c] + mean[c]\n",
    "plt.hist(img0.numpy().flatten(), bins=30, color='gray', alpha=0.7)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> dict[str, float]:\n",
    "    model.train()\n",
    "    n_total = len(train_dataloader.dataset)\n",
    "    n_batches = len(train_dataloader)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X) \n",
    "        train_loss: torch.Tensor = loss_fn(pred, y) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = train_loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{n_total:>5d}]\")\n",
    "\n",
    "    avg_loss = running_loss / n_batches\n",
    "    return {\"avg_loss\": avg_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test() -> dict[str, float]:\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return {\"test_loss\": test_loss, \"test_accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_info = train()\n",
    "    test_info = test()\n",
    "\n",
    "    train_losses.append(train_info[\"avg_loss\"])\n",
    "    test_losses.append(test_info[\"test_loss\"])\n",
    "    test_accuracies.append(test_info[\"test_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
