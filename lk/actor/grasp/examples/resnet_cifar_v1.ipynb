{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data from disk\n",
    "CIFAR = \"CIFAR10\"   # or \"CIFAR100\"\n",
    "num_classes = 10 if CIFAR == \"CIFAR10\" else 100\n",
    "Dataset = getattr(datasets, CIFAR)\n",
    "\n",
    "data_path = \"/home/bam/bam_ws/src/bam_brain/bam_gym/data_downloads\"\n",
    "\n",
    "# Load CIFAR without normalization so we can compute stats\n",
    "dataset = Dataset(\n",
    "    root=data_path, train=True, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False, num_workers=4)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "n_samples = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    # images shape: [batch, channels, height, width]\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)  # [batch, C, H*W]\n",
    "    \n",
    "    mean += images.mean(2).sum(0)   # sum over batch\n",
    "    std += images.std(2).sum(0)\n",
    "    n_samples += batch_samples\n",
    "\n",
    "mean /= n_samples\n",
    "std /= n_samples\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data loaders with transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_dataset = Dataset(root=data_path, train=True, download=True, transform=train_transforms)\n",
    "test_dataset  = Dataset(root=data_path, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_dataloader  = torch.utils.data.DataLoader(test_dataset,  batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Why are larger batch sizes used for testing?\n",
    "\n",
    "# At testing we are doing stochastic gradient descent (stochastic, beacuse we estimating the gradient, as its not the full datset). This bit of noise in practice doesn't seem to harm, and can actually be helpful\n",
    "# Rule of thumb is to load the largest batch size that fits into GPU memory, while still leaving room for gradients. Its a hyper parameter to tune though, who knows what works best in practice for the loss landscape\n",
    "\n",
    "# For testing, reqs. are different. No gradients, so less memory requirment, and we just want to go through as fast as possible. So generally you can use a larger batch size.\n",
    "\n",
    "# See chat: https://chatgpt.com/c/68b58cae-31f4-8332-980b-5ceb5065d7ce\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "def resnet18_cifar(num_classes=10):\n",
    "    model = tv.models.resnet18(weights=None)  # start from ImageNet-style architecture\n",
    "    # Replace the 7x7 stride-2 conv + maxpool with 3x3 stride-1 and no pool:\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    # Adjust classifier head:\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18_cifar(num_classes).to(device)\n",
    "summary(model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model with pretrained weights\n",
    "\n",
    "def resnet18_cifar(num_classes=10, pretrained=True):\n",
    "    # Load pretrained weights from ImageNet\n",
    "    model = tv.models.resnet18(weights=\"IMAGENET1K_V1\" if pretrained else None)\n",
    "\n",
    "    # ---- 1) Replace the first conv (7x7 -> 3x3, stride 2 -> stride 1) ----\n",
    "    old_conv = model.conv1\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    if pretrained:\n",
    "        # Adapt pretrained conv1 weights (old shape: [64, 3, 7, 7])\n",
    "        with torch.no_grad():\n",
    "            w = old_conv.weight\n",
    "            # Center-crop the 7x7 kernels to 3x3\n",
    "            model.conv1.weight.copy_(w[:, :, 2:5, 2:5])\n",
    "\n",
    "    # ---- 2) Remove the maxpool (not needed for CIFAR’s 32x32 images) ----\n",
    "    model.maxpool = nn.Identity()\n",
    "\n",
    "    # ---- 3) Replace classifier ----\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18_cifar(num_classes=10, pretrained=True).to(device)\n",
    "summary(model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model with no freezing\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:30} requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Layers\n",
    "def freeze_module(module, freeze_bn: bool = False):\n",
    "    \"\"\"\n",
    "    Freeze a module by disabling gradients for its parameters.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The PyTorch module to freeze.\n",
    "        freeze_bn (bool): If True, also freeze BatchNorm layers inside:\n",
    "            - set requires_grad=False for gamma/beta\n",
    "            - put BN in eval() mode (stops running stats updates)\n",
    "    \"\"\"\n",
    "    # freeze all params in this module\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if freeze_bn:\n",
    "        # walk over all nested submodules\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = False\n",
    "                m.eval()\n",
    "\n",
    "freeze_module(model.layer1, freeze_bn=True)\n",
    "freeze_module(model.layer2, freeze_bn=True)\n",
    "freeze_module(model.layer3, freeze_bn=True)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:30} requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that BN layers are frozen properly\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n",
    "        print(f\"{name:30} training={module.training}  \"\n",
    "              f\"weight_grad={module.weight.requires_grad}  \"\n",
    "              f\"bias_grad={module.bias.requires_grad}\")\n",
    "        \n",
    "\"\"\"\n",
    "training=False means it’s in eval mode (running stats not updating).\n",
    "\n",
    "requires_grad=False for weight/bias means γ/β won’t be trained.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    model.train()\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            total += x.size(0)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct / total):>0.1f}%, Avg loss: {loss_sum / total:>8f} \\n\")\n",
    "\n",
    "    return loss_sum / total, correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_one_epoch()\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
