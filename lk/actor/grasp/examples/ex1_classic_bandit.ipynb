{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok what is the simplest thing I can start with? Just a multi armed bandit...\n",
    "\n",
    "# this should just look like a very simple script....\n",
    "\n",
    "# What is my goal? Have a clear idea and you may get what you need.\n",
    "# Utlimate goal is to get up robot arms, have them just attempt to grasp objects, and then come back 1 week later, and their success rate is near 100%...\n",
    "\n",
    "# Idea is to build up from simple to complex grasps and motion...\n",
    "\n",
    "\n",
    "# What does good look like? Maximise reward, maximise success rate, fast computation, good deployment performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_config = dict()\n",
    "# env_config = dict()\n",
    "# policy = Policy.make(policy_config) # For now lets just directly construct it\n",
    "# env = Env.make(env_config) # I can just use the gym registration system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions of the RL problem\n",
    "\n",
    "- Action space size (larger is harder)\n",
    "- State space size (large is harder)\n",
    "- Episode Time horizion (further away is harder, credit assignment problem)\n",
    "- History/Context Req./Markov property (More context is harder, essentially greater state space, blind spots in driving, you cannot forget about the car)\n",
    "- World Dynamics -> are they constatly changing? Is it easy to predict? Are there other agents?\n",
    "\n",
    "\n",
    "How to build up?\n",
    "\n",
    "We start with very simple gym and envs ex1_classic_bandit -> at some point we get to ex20 type thing :)\n",
    "\n",
    "How is this different than the full robotics problem?\n",
    "\n",
    "1. Here we assume we know the full state, in reality we sensors are noisey, kinematic chains are not accurate, etc.\n",
    "2. We are not worried about execution* that is an offline dataset or simulator\n",
    "\n",
    "action = policy(state)\n",
    "\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "Here was my idea to deal with the delayed rewards! don't return until you have the reward, so you can wait inside the environment, everything else is outside your control, so this make sense..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am getting import errors for some bam gym py types etc...\n",
    "That is not good! this really should just be in numpy land, etc... clear away all of these distractions please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import bam_gym # import to that __init__.py is run to register envs\n",
    "from bam_gym import print_reset, print_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Square numbers are easy to see as square\n",
    "n_arms = 4**2\n",
    "env = gym.make(\"bam_local/ClassicBandit\", render_mode=\"human\", clip=10, n_arms=n_arms)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Reset:\", env.reset())\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "print_reset(state, info)\n",
    "\n",
    "def reset_env(env, terminated, truncated, info) -> bool:\n",
    "    if terminated or truncated:\n",
    "        if env.metadata['autoreset_mode'] != gym.vector.AutoresetMode.SAME_STEP:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    # action = policy(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print_step(action, state, reward, terminated, truncated, info, i)\n",
    "\n",
    "    if reset_env(env, terminated, truncated, info):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can use a super duper simple policy to beat this, essentially just average the rewards to date, and then pick the highest action with epislon greedy probability\n",
    "# If you want to get a very accuarte understanding of the world, then you can continue to do random exploration, but to actually maximise reward you want to exploit as well!\n",
    "\n",
    "class AveragePolicy():\n",
    "    def __init__(self, n_actions, epsilon=0.2):\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.iter = 0\n",
    "        self.attempts = np.ones(n_actions) # avoid divide by zero\n",
    "        self.rewards = np.ones(n_actions)/n_actions\n",
    "        self.W = self.rewards / self.attempts\n",
    "        # self.W = np.ones(n_actions)/n_actions\n",
    "\n",
    "\n",
    "    def __call__(self, state):\n",
    "        self.iter += 1\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.W)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        self.attempts[action] += 1\n",
    "        self.rewards[action] += reward\n",
    "        self.W = self.rewards / self.attempts\n",
    "\n",
    "\n",
    "# Be a little fancy and calculing in a moving average way :)\n",
    "#     def update(self, action, reward):\n",
    "#         self.attempts[action] += 1\n",
    "#         self.rewards[action] += reward\n",
    "#         self.W = self.W * (1 - self.alpha) + self.rewards / self.attempts * self.alpha\n",
    "\n",
    "\n",
    "policy = AveragePolicy(n_actions=n_arms, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "# policy.epsilon = 0.0 # Manually tune epsilon \n",
    "\n",
    "for i in range(1000):\n",
    "    action = policy(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    # print_step(action, state, reward, terminated, truncated, info, i)\n",
    "    policy.update(action, reward)\n",
    "    reward_history.append(reward)\n",
    "\n",
    "    if reset_env(env, terminated, truncated, info):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(policy.attempts)\n",
    "print(policy.rewards)\n",
    "\n",
    "width = 0.4\n",
    "x = np.arange(n_arms)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plt.bar(x - width/2, env.unwrapped.arm_thresholds, width, alpha=0.7, label=\"Actual\")\n",
    "plt.bar(x + width/2, policy.W, width, alpha=0.7, label=\"Estimated\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Arm')\n",
    "plt.ylabel('Threshold')\n",
    "plt.title('Classic Bandit Arms: Thresholds')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history, label=\"Reward\")\n",
    "\n",
    "# Compute moving average\n",
    "window_size = 50\n",
    "if len(reward_history) >= window_size:\n",
    "    moving_avg = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(reward_history)), moving_avg, label=f\"Moving Avg ({window_size})\", color='orange')\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward History and Moving Average\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One idea is to view as image... better I think to compare agianst the ground turth bar chart though!\n",
    "\n",
    "W = policy.W  # assume it's a 1D numpy array\n",
    "n = W.shape[0]\n",
    "\n",
    "# Find next perfect square\n",
    "square_dim = int(np.ceil(np.sqrt(n)))\n",
    "padded_len = square_dim ** 2\n",
    "\n",
    "# Pad with zeros (or another value if you want, e.g. np.nan)\n",
    "W_padded = np.pad(W, (0, padded_len - n), mode=\"constant\", constant_values=np.nan)\n",
    "\n",
    "# Reshape to square\n",
    "W_square = W_padded.reshape(square_dim, square_dim)\n",
    "print(W.shape)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(W_square, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "\n",
    "# Show ticks at integer positions (pixel centers)\n",
    "ax.set_xticks(np.arange(W_square.shape[1]))\n",
    "ax.set_yticks(np.arange(W_square.shape[0]))\n",
    "\n",
    "# Add gridlines between pixels if you like\n",
    "ax.set_xticks(np.arange(-0.5, W_square.shape[1], 1), minor=True)\n",
    "ax.set_yticks(np.arange(-0.5, W_square.shape[0], 1), minor=True)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bam_ws_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
